{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import string\n",
    "import re\n",
    "import codecs\n",
    "import collections\n",
    "import sys\n",
    "from itertools import islice, tee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Языки\n",
    "Языки, для которых реализовано распознавание - горномарийский, якутский, татарский, чувашский:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "langs = ('mrj', 'sah', 'tt', 'cv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Обучение\n",
    "Функция для скачивания статей из википедии (взято [отсюда](https://github.com/ElizavetaKuzmenko/Programming-and-computer-instruments/blob/master/nlp3year/LangDetect_3year.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_texts_for_lang(lang, n=10):\n",
    "    wikipedia.set_lang(lang)\n",
    "    wiki_content = []\n",
    "    pages = wikipedia.random(n)\n",
    "    for page_name in pages:\n",
    "        try:\n",
    "            page = wikipedia.page(page_name)\n",
    "        except wikipedia.exceptions.WikipediaException:\n",
    "            print('Skipping page {}'.format(page_name))\n",
    "            continue\n",
    "\n",
    "        wiki_content.append('{}\\n{}'.format(page.title, page.content.replace('==', '')))\n",
    "\n",
    "    return wiki_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем обучающие статьи для каждого языка (взято [отсюда](https://github.com/ElizavetaKuzmenko/Programming-and-computer-instruments/blob/master/nlp3year/LangDetect_3year.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrj 100\n",
      "Skipping page Бэстээх (Хаҥалас улууһа)\n",
      "sah 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\Андрей\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page 79 (мәгънәләр)\n",
      "Skipping page 964 (мәгънәләр)\n",
      "Skipping page 104 (мәгънәләр)\n",
      "Skipping page 279 (мәгънәләр)\n",
      "tt 96\n",
      "Skipping page Çăхан (пĕлтерĕшĕсем)\n",
      "cv 99\n"
     ]
    }
   ],
   "source": [
    "wiki_texts = {}\n",
    "for lang in langs:\n",
    "    wiki_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(wiki_texts[lang]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для токенизации (специфично для текстов на кириллице - удаляется вся стандартная латиница):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return filter(None,re.split('[ —–'+string.punctuation+'a-zA-Z0-9]+',text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для удаления повторов из словарей (in-place):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def symmetric_difference_dicts(d,d_keys):\n",
    "    not_repeated = set(d[d_keys[0]].keys())\n",
    "    for k in d_keys[1:]:\n",
    "        not_repeated = not_repeated ^ set(d[k].keys())\n",
    "    for k in d_keys:\n",
    "        d[k] = {x:d[k][x] for x in not_repeated if x in d[k]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция, возвращающая списки самых частотных элементов для языков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#most frequent words in each language\n",
    "def get_most_freq(d,d_keys,num):\n",
    "    mostfreq = {}\n",
    "    for k in d_keys:\n",
    "        mostfreq[k] = set(sorted(d[k], key=lambda w: d[k][w], reverse=True)[:num])\n",
    "    return mostfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для получения списков самых частотных слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_frequent_words(texts,langs):\n",
    "    freqs = {}\n",
    "    for lang in wiki_texts:\n",
    "        corpus = wiki_texts[lang]\n",
    "        freqs[lang] = collections.defaultdict(lambda: 0)\n",
    "        for article in corpus:\n",
    "            for word in tokenize(article.replace('\\n', '').lower()):\n",
    "                freqs[lang][word] += 1\n",
    "    \n",
    "    symmetric_difference_dicts(freqs,langs)\n",
    "   \n",
    "    mostfreq = get_most_freq(freqs,langs,100)\n",
    "\n",
    "    return mostfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для определения языка текста словарным методом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dict_method(text,mostfreq):\n",
    "    sums = {}\n",
    "    for lang in mostfreq:\n",
    "        sums[lang] = len([word for word in tokenize(text.replace('\\n', '').lower()) if word in mostfreq[lang]])\n",
    "    return sorted(sums, key=lambda w: sums[w], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составление n-грамм (взято [отсюда](https://github.com/ElizavetaKuzmenko/Programming-and-computer-instruments/blob/master/nlp3year/LangDetect_3year.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_ngrams(text):\n",
    "    N = 3 # задаем длину n-граммы\n",
    "    ngrams = zip(*(islice(seq, index, None) for index, seq in enumerate(tee(text, N))))\n",
    "    ngrams = [''.join(x) for x in ngrams]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для получения списков самых частотных n-грамм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_frequent_ngrams(texts,langs):\n",
    "    freqs = {}\n",
    "    for lang in wiki_texts:\n",
    "        corpus = wiki_texts[lang]\n",
    "        freqs[lang] = collections.defaultdict(lambda: 0)\n",
    "        for article in corpus:\n",
    "            for ngram in make_ngrams(article.replace('\\n', '').lower()):\n",
    "                freqs[lang][ngram] += 1\n",
    "    \n",
    "    symmetric_difference_dicts(freqs,langs)\n",
    "   \n",
    "    mostfreq = get_most_freq(freqs,langs,300)\n",
    "\n",
    "    return mostfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для определения языка текста методом n-грамм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngram_method(text,mostfreq):\n",
    "    sums = {}\n",
    "    for lang in mostfreq:\n",
    "        sums[lang] = len([ngram for ngram in make_ngrams(text.replace('\\n', '').lower()) if ngram in mostfreq[lang]])\n",
    "    return sorted(sums, key=lambda w: sums[w], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим частотные слова и n-граммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mostfreqwords = most_frequent_words(wiki_texts,langs)\n",
    "mostfreqngrams = most_frequent_ngrams(wiki_texts,langs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Предсказание\n",
    "Функция, открывающая файл и предсказывающая для него язык:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_lang(filename,mfw,mfn):\n",
    "    text = open(filename,'r',encoding='utf-8-sig').read()\n",
    "    print('dict method:',dict_method(text,mfw))\n",
    "    print('ngram method:',ngram_method(text,mfn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предскажем язык текста из файла text.txt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict method: mrj\n",
      "ngram method: mrj\n"
     ]
    }
   ],
   "source": [
    "predict_lang('text.txt',mostfreqwords,mostfreqngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Язык определен верно.\n",
    "### 3. Отчет\n",
    "Лучше работает метод n-грамм. Тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrj 100\n",
      "sah 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Андрей\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 184 of the file C:\\Users\\Андрей\\Anaconda3\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping page TR\n",
      "Skipping page 298 (мәгънәләр)\n",
      "Skipping page Аксумла\n",
      "tt 97\n",
      "cv 100\n"
     ]
    }
   ],
   "source": [
    "# test texts\n",
    "test_texts = {}\n",
    "for lang in langs:\n",
    "    test_texts[lang] = get_texts_for_lang(lang, 100)\n",
    "    print(lang, len(test_texts[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test\n",
    "def test(textdict,mfw,mfn):\n",
    "    errors_dict = 0\n",
    "    errors_ngrams = 0\n",
    "    texts = 0\n",
    "    for lang in textdict:\n",
    "        corpus = textdict[lang]\n",
    "        for article in corpus:\n",
    "            texts += 1\n",
    "            if lang != dict_method(article,mfw):\n",
    "                errors_dict += 1\n",
    "            if lang != ngram_method(article,mfn):\n",
    "                errors_ngrams += 1\n",
    "    print('dict error rate:',errors_dict/texts)\n",
    "    print('ngram error rate:',errors_ngrams/texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict error rate: 0.022670025188916875\n",
      "ngram error rate: 0.0025188916876574307\n"
     ]
    }
   ],
   "source": [
    "test(test_texts,mostfreqwords,mostfreqngrams)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
